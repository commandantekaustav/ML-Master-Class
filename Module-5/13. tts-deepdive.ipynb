{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T10:34:46.790723Z","iopub.execute_input":"2025-04-03T10:34:46.791086Z","iopub.status.idle":"2025-04-03T10:34:47.227275Z","shell.execute_reply.started":"2025-04-03T10:34:46.791058Z","shell.execute_reply":"2025-04-03T10:34:47.226261Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/creditcardfraud/creditcard.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Train-Test Split & Cross-Validation\n\n### 1. Introduction: Model Evaluation, Generalization, and Overfitting\n\n* **Model Evaluation:**\n   \n    * The cornerstone of machine learning, model evaluation quantifies a model's performance on a given dataset.\n    * It's not just about getting a score; it's about understanding how well our model is likely to perform in the real world.\n* **Generalization:**\n   \n    * The ultimate goal of any machine learning model.\n    * Generalization is the model's capacity to accurately predict outcomes on *unseen* data, data it wasn't trained on.\n    * A model with good generalization is robust and reliable.\n* **Overfitting:**\n   \n    * The nemesis of generalization.\n    * Overfitting occurs when a model learns the training data *too* well, capturing noise and idiosyncrasies instead of the underlying patterns.\n    * An overfit model performs brilliantly on training data but miserably on new data.\n    * Evaluation techniques are crucial for detecting and mitigating overfitting.\n* **Mathematical Significance:**\n   \n    * Model evaluation is deeply rooted in statistical learning theory, which provides a framework for quantifying the uncertainty in model predictions.\n    * Concepts like bias-variance tradeoff, confidence intervals, and statistical hypothesis testing are fundamental to rigorous model evaluation.\n\n### 2. Train-Test Split: The Basic Necessity\n\n* **Importance:**\n   \n    * The train-test split is the most basic yet essential technique to assess generalization.\n    * It simulates the real-world scenario where a model is trained on historical data and deployed to make predictions on new, incoming data.\n    * By holding out a portion of the data, we get an unbiased estimate of how the model will perform in practice.\n* **Implementation:**\n   \n    * The data is partitioned into two mutually exclusive subsets:\n        * **Training Set:** The larger subset, typically 70-80% of the data, used to train the model. The model learns the patterns and relationships within this data.\n        * **Testing Set:** The smaller subset, typically 20-30% of the data, held aside and used *only* to evaluate the model's final performance. This set acts as a proxy for unseen data.\n    * The split is ideally performed *randomly* to ensure both sets have a similar statistical distribution.\n    * Scikit-learn's `train_test_split` function simplifies this process, offering control over the split ratio and randomization.\n* **Demonstrating the Impact:**\n   \n    * We'll train a simple model (e.g., Logistic Regression) *without* and *with* a train-test split to highlight the difference in evaluation.\n* **Mathematical/Technical Significance:**\n   \n    * The train-test split provides an *estimate* of the generalization error.\n    * If we train and evaluate on the same data, our performance metrics will be optimistically biased, reflecting the model's ability to *memorize* rather than *generalize*.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the Credit Card Fraud Detection dataset (replace with your path)\ndata = pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")\ndata.drop('Time', axis=1, inplace=True) # Drop 'Time' for simplicity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T10:42:04.802998Z","iopub.execute_input":"2025-04-03T10:42:04.803367Z","iopub.status.idle":"2025-04-03T10:42:07.421661Z","shell.execute_reply.started":"2025-04-03T10:42:04.803341Z","shell.execute_reply":"2025-04-03T10:42:07.420781Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"X = data.drop('Class', axis=1)\ny = data['Class']\n\n# 1. Training and Evaluating WITHOUT Train-Test Split (Illustrating Overfitting)\nmodel_no_split = LogisticRegression(solver='liblinear')\nmodel_no_split.fit(X, y)\ny_pred_no_split = model_no_split.predict(X) # Predict on the *same* data\nprint(\"--- Evaluation WITHOUT Train-Test Split ---\")\nprint(\"Accuracy:\", accuracy_score(y, y_pred_no_split))\nprint(\"Classification Report:\\n\", classification_report(y, y_pred_no_split))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T10:42:11.288429Z","iopub.execute_input":"2025-04-03T10:42:11.288743Z","iopub.status.idle":"2025-04-03T10:42:20.133387Z","shell.execute_reply.started":"2025-04-03T10:42:11.288719Z","shell.execute_reply":"2025-04-03T10:42:20.132470Z"}},"outputs":[{"name":"stdout","text":"--- Evaluation WITHOUT Train-Test Split ---\nAccuracy: 0.9992029690281489\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00    284315\n           1       0.88      0.62      0.73       492\n\n    accuracy                           1.00    284807\n   macro avg       0.94      0.81      0.86    284807\nweighted avg       1.00      1.00      1.00    284807\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# 2. Training and Evaluating WITH Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Split data\nmodel_with_split = LogisticRegression(solver='liblinear')\nmodel_with_split.fit(X_train, y_train)\ny_pred_with_split = model_with_split.predict(X_test) # Predict on the *test* data\nprint(\"\\n--- Evaluation WITH Train-Test Split ---\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_with_split))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_with_split))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T10:45:06.385166Z","iopub.execute_input":"2025-04-03T10:45:06.385522Z","iopub.status.idle":"2025-04-03T10:45:14.640419Z","shell.execute_reply.started":"2025-04-03T10:45:06.385496Z","shell.execute_reply":"2025-04-03T10:45:14.639354Z"}},"outputs":[{"name":"stdout","text":"\n--- Evaluation WITH Train-Test Split ---\nAccuracy: 0.9991046662687406\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     56864\n           1       0.86      0.57      0.69        98\n\n    accuracy                           1.00     56962\n   macro avg       0.93      0.79      0.84     56962\nweighted avg       1.00      1.00      1.00     56962\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"* **Expected Outcome:**\n   \n    * The model trained and evaluated *without* a train-test split will likely show very high accuracy and excellent classification metrics. This is misleading! It's simply memorizing the training data.\n    * The model trained *with* a train-test split will show a more realistic (and often lower) accuracy on the test set. This is a better reflection of how the model will perform on new data.\n\n> This is a demo dataset hence we can always expect high accurary however it still makes some difference as you observed.\n>\n> \n* **Key Takeaway:**\n   \n    * The difference between the two evaluations highlights the importance of the train-test split in providing an unbiased estimate of generalization performance.\n\n### 3. Cross-Validation: Robust Evaluation\n\n* **Limitations of Train-Test Split:**\n   \n    * While essential, a single train-test split has limitations:\n        * **Variability:** The performance estimate can be sensitive to how the data is split, especially with smaller datasets. A different random split might yield a different result.\n        * **Data Waste:** The test set is held out entirely from training, meaning we're not using all available data to train the model. This can be a concern when data is scarce.\n* **Cross-Validation to the Rescue:**\n   \n    * Cross-validation is a powerful technique to overcome these limitations.\n    * It provides a *more robust* and *less biased* estimate of model performance by averaging performance across multiple train-test splits.\n    * It also utilizes *all* data for both training and evaluation, addressing the data waste issue.\n* **k-Fold Cross-Validation:**\n   \n    * The most common type of cross-validation.\n    * The data is divided into *k* equal-sized \"folds.\"\n    * The model is trained and evaluated *k* times.\n    * In each \"fold,\" one fold is held out as the test set, and the remaining *k-1* folds are used as the training set.\n    * The final performance is the *average* of the performance across the *k* folds.\n    * Common values for *k* are 5 and 10.\n* **Stratified k-Fold Cross-Validation:**\n   \n    * A variant of k-fold, crucial for *imbalanced* datasets (like our fraud data).\n    * It ensures that *each fold* has approximately the *same proportion* of samples from each class as the original dataset.\n    * This prevents a scenario where some folds have very few or no examples of the minority class (e.g., fraud), which would lead to unreliable evaluation.\n* **Mathematical/Technical Significance:**\n   \n    * Cross-validation provides a *distribution* of performance metrics (e.g., a distribution of accuracy scores across the k folds), giving us a better sense of the model's variability.\n    * The average performance from cross-validation is a *more reliable* estimate of the generalization error than a single train-test split.\n    * Stratified k-fold maintains the class distribution, which is essential for reliable evaluation in classification problems with imbalanced classes.\n\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, make_scorer\n\n# Reload data (to be safe)\ndata = pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")\ndata.drop('Time', axis=1, inplace=True)\nX = data.drop('Class', axis=1)\ny = data['Class']\n\nmodel = LogisticRegression(solver='liblinear') # Our model\n\n# 1. K-Fold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42) # 5 folds, shuffle data\n\nprint(\"\\n--- K-Fold Cross-Validation ---\")\nauc_scores_kf = cross_val_score(model, X, y, cv=kf, scoring='roc_auc') # AUC for each fold\nprint(\"AUC Scores (K-Fold):\", auc_scores_kf)\nprint(\"Mean AUC (K-Fold):\", auc_scores_kf.mean())\nprint(\"AUC Std Dev (K-Fold):\", auc_scores_kf.std()) # Variability\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T10:46:10.083938Z","iopub.execute_input":"2025-04-03T10:46:10.084301Z","iopub.status.idle":"2025-04-03T10:46:51.549456Z","shell.execute_reply.started":"2025-04-03T10:46:10.084274Z","shell.execute_reply":"2025-04-03T10:46:51.548460Z"}},"outputs":[{"name":"stdout","text":"\n--- K-Fold Cross-Validation ---\nAUC Scores (K-Fold): [0.97630688 0.97969451 0.96133088 0.98437416 0.97225807]\nMean AUC (K-Fold): 0.9747929014598146\nAUC Std Dev (K-Fold): 0.007820099053305354\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# 2. Stratified K-Fold Cross-Validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nprint(\"\\n--- Stratified K-Fold Cross-Validation ---\")\nauc_scores_skf = cross_val_score(model, X, y, cv=skf, scoring='roc_auc')\nprint(\"AUC Scores (Stratified K-Fold):\", auc_scores_skf)\nprint(\"Mean AUC (Stratified K-Fold):\", auc_scores_skf.mean())\nprint(\"AUC Std Dev (Stratified K-Fold):\", auc_scores_skf.std())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T10:46:51.550523Z","iopub.execute_input":"2025-04-03T10:46:51.550796Z","iopub.status.idle":"2025-04-03T10:47:26.804080Z","shell.execute_reply.started":"2025-04-03T10:46:51.550765Z","shell.execute_reply":"2025-04-03T10:47:26.803077Z"}},"outputs":[{"name":"stdout","text":"\n--- Stratified K-Fold Cross-Validation ---\nAUC Scores (Stratified K-Fold): [0.97758177 0.98356265 0.97903428 0.96609036 0.9652525 ]\nMean AUC (Stratified K-Fold): 0.9743043141085049\nAUC Std Dev (Stratified K-Fold): 0.0073244197100807255\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"* **Expected Outcome:**\n   \n    * Cross-validation will provide a *more consistent* estimate of the model's performance compared to a single train-test split.\n    * Stratified k-fold will be particularly important for the Credit Card Fraud dataset. Because it is heavily imbalanced, stratified k-fold will likely provide a more reliable estimate of the model's ability to detect fraud.\n    * The standard deviation of the cross-validation scores will give us a measure of the variability in the model's performance across different folds. Lower standard deviation is desirable, indicating a more stable model.\n* **Key Takeaways:**\n   \n    * Cross-validation is a powerful tool for robust model evaluation.\n    * Stratification is essential for imbalanced datasets.\n    * Cross-validation provides a more complete picture of model performance than a single train-test split.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}